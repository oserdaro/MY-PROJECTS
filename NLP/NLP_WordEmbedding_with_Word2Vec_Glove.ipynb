{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Word_embedding using Word2Vec and Glove\n",
    "\n",
    "Word embedding is the expression of a word or phrase as a flat vector. These vectors are learned by the model to reflect similarities between words. In this way, AI models can better understand the relationships between words\n",
    "\n",
    "Word embedding is learned using text data. For example, using a text corpus (large text database), it learns for each word its relationship with the words around it. This relationship is defined as the co-occurrence frequency between the word and its surrounding words. Then, using this co-occurrence data, a vector is created for each word. These vectors are designed to reflect the similarities between the meanings of the words.\n",
    "\n",
    "For example, the words “teacher”, “student”, “grading” often appear in the same texts and have similar meanings, so their resulting vectors will be close to each other. However, the words “teacher” and “orange” do not often appear in the same texts and have different meanings, so their vectors will be far apart."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Representation (Feature Extraction for word embeddings)\n",
    "\n",
    "Each element in the word embbeding vector, which usually takes a value between -1 and +1 (in some pre-trained models this value can be greater than -1 and +1), is called a feature representation. The process of generating these feature representations by the model is called feature extraction.\n",
    "\n",
    "In ML, feature extraction is a method used to identify features or attributes present in the dataset. These features allow them to express the data in the dataset in a meaningful way. These features are manually selected from the dataset.\n",
    "\n",
    "In DL, feature extraction is a method used to learn features from the dataset. The methods used for this method include artificial neural networks such as Convolutional Neural Network (CNN) and Recurrent Neural Network (RNN). These neural networks automatically learn features from the dataset, and these features allow them to express the data in the dataset in a more meaningful way.\n",
    "\n",
    "In summary, in ML, features are manually selected by the user, while in DL, features are detected and learned by the automated model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim\n",
      "  Downloading gensim-4.3.2-cp311-cp311-win_amd64.whl.metadata (8.5 kB)\n",
      "Requirement already satisfied: numpy>=1.18.5 in c:\\users\\serda\\anaconda3\\envs\\da\\lib\\site-packages (from gensim) (1.26.0)\n",
      "Requirement already satisfied: scipy>=1.7.0 in c:\\users\\serda\\anaconda3\\envs\\da\\lib\\site-packages (from gensim) (1.11.3)\n",
      "Collecting smart-open>=1.8.1 (from gensim)\n",
      "  Downloading smart_open-7.0.4-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting wrapt (from smart-open>=1.8.1->gensim)\n",
      "  Downloading wrapt-1.16.0-cp311-cp311-win_amd64.whl.metadata (6.8 kB)\n",
      "Downloading gensim-4.3.2-cp311-cp311-win_amd64.whl (24.0 MB)\n",
      "   ---------------------------------------- 0.0/24.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.2/24.0 MB 5.3 MB/s eta 0:00:05\n",
      "   - -------------------------------------- 0.7/24.0 MB 9.2 MB/s eta 0:00:03\n",
      "   -- ------------------------------------- 1.3/24.0 MB 10.2 MB/s eta 0:00:03\n",
      "   --- ------------------------------------ 1.8/24.0 MB 10.6 MB/s eta 0:00:03\n",
      "   --- ------------------------------------ 2.4/24.0 MB 10.8 MB/s eta 0:00:03\n",
      "   ---- ----------------------------------- 2.9/24.0 MB 10.9 MB/s eta 0:00:02\n",
      "   ----- ---------------------------------- 3.4/24.0 MB 10.7 MB/s eta 0:00:02\n",
      "   ------ --------------------------------- 4.0/24.0 MB 11.0 MB/s eta 0:00:02\n",
      "   ------- -------------------------------- 4.5/24.0 MB 11.1 MB/s eta 0:00:02\n",
      "   -------- ------------------------------- 5.0/24.0 MB 11.1 MB/s eta 0:00:02\n",
      "   -------- ------------------------------- 5.4/24.0 MB 11.1 MB/s eta 0:00:02\n",
      "   --------- ------------------------------ 5.9/24.0 MB 10.8 MB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 6.5/24.0 MB 11.0 MB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 7.1/24.0 MB 11.1 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 7.6/24.0 MB 11.1 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 8.2/24.0 MB 11.1 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 8.7/24.0 MB 11.1 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 9.1/24.0 MB 11.2 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 9.8/24.0 MB 11.2 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 10.3/24.0 MB 11.3 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 10.9/24.0 MB 11.5 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 11.4/24.0 MB 11.5 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 12.0/24.0 MB 11.5 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 12.5/24.0 MB 11.5 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 13.0/24.0 MB 11.5 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 13.6/24.0 MB 11.5 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 14.2/24.0 MB 11.5 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 14.6/24.0 MB 11.3 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 15.2/24.0 MB 11.5 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 15.7/24.0 MB 12.1 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 16.2/24.0 MB 11.7 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 16.8/24.0 MB 11.7 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 17.0/24.0 MB 11.7 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 17.0/24.0 MB 11.7 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 18.4/24.0 MB 11.7 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 18.9/24.0 MB 11.7 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 19.4/24.0 MB 11.7 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 19.9/24.0 MB 11.5 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 20.5/24.0 MB 11.7 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 21.1/24.0 MB 11.5 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 21.5/24.0 MB 11.5 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 22.1/24.0 MB 11.5 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 22.7/24.0 MB 11.5 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 23.2/24.0 MB 11.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  23.8/24.0 MB 11.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.0/24.0 MB 11.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.0/24.0 MB 11.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.0/24.0 MB 11.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 24.0/24.0 MB 10.1 MB/s eta 0:00:00\n",
      "Downloading smart_open-7.0.4-py3-none-any.whl (61 kB)\n",
      "   ---------------------------------------- 0.0/61.2 kB ? eta -:--:--\n",
      "   ---------------------------------------- 61.2/61.2 kB ? eta 0:00:00\n",
      "Downloading wrapt-1.16.0-cp311-cp311-win_amd64.whl (37 kB)\n",
      "Installing collected packages: wrapt, smart-open, gensim\n",
      "Successfully installed gensim-4.3.2 smart-open-7.0.4 wrapt-1.16.0\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim\n",
    "# conda install -c conda-forge gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "import pandas as pd\n",
    "from gensim.models import Word2Vec\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\serda\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\serda\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\serda\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\serda\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>news</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>iran devlet televizyonu ülkedeki eyaletin sind...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gösterilerde fitnecilere ölüm münafıklara ölüm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dini lider ali hamaney ve cumhurbaşkanı mahmud...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>musevi ye ölüm ve idam idam sloganları duyuldu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>muhalefet liderleri kaçtı mı aşure günü yaşana...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411520</th>\n",
       "      <td>dışişleri bakanlığı ndan yapılan yazılı açıkla...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411521</th>\n",
       "      <td>açıklamada abd nin ankara büyükelçiliği ve ist...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411522</th>\n",
       "      <td>seyahat uyarısı güncelleme kararının temmuz da...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411523</th>\n",
       "      <td>amerikalı turistlerin açıkça türkiye deki ulus...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411524</th>\n",
       "      <td>abd dışişleri bakanlığı seyahat uyarısı açıkla...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>411525 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     news\n",
       "0       iran devlet televizyonu ülkedeki eyaletin sind...\n",
       "1       gösterilerde fitnecilere ölüm münafıklara ölüm...\n",
       "2       dini lider ali hamaney ve cumhurbaşkanı mahmud...\n",
       "3         musevi ye ölüm ve idam idam sloganları duyuldu \n",
       "4       muhalefet liderleri kaçtı mı aşure günü yaşana...\n",
       "...                                                   ...\n",
       "411520  dışişleri bakanlığı ndan yapılan yazılı açıkla...\n",
       "411521  açıklamada abd nin ankara büyükelçiliği ve ist...\n",
       "411522  seyahat uyarısı güncelleme kararının temmuz da...\n",
       "411523  amerikalı turistlerin açıkça türkiye deki ulus...\n",
       "411524  abd dışişleri bakanlığı seyahat uyarısı açıkla...\n",
       "\n",
       "[411525 rows x 1 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('newspaper.zip', names = [\"news\"])\n",
    "df\n",
    "\n",
    "# pandas' read_csv function can also read zip files. \n",
    "# Since the data (corpus) we zip is a txt file, there are no feature name(s) specified in the file. Therefore we use the names parameter for feature naming.\n",
    "# If the names parameter is not used, the text in the first line is assigned as feature names by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'iran devlet televizyonu ülkedeki eyaletin sinde yapılan reformcuları protesto amaçlı yürüyüşlere milyonlarca kişinin katıldığını bildirdi '"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.news[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['iran',\n",
       " 'devlet',\n",
       " 'televizyonu',\n",
       " 'ülkedeki',\n",
       " 'eyaletin',\n",
       " 'sinde',\n",
       " 'yapılan',\n",
       " 'reformcuları',\n",
       " 'protesto',\n",
       " 'amaçlı',\n",
       " 'yürüyüşlere',\n",
       " 'milyonlarca',\n",
       " 'kişinin',\n",
       " 'katıldığını',\n",
       " 'bildirdi']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(df.news[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['iran', 'devlet', 'televizyonu', 'ülkedeki', 'eyaletin', 'sinde', 'yapılan', 'reformcuları', 'protesto', 'amaçlı', 'yürüyüşlere', 'milyonlarca', 'kişinin', 'katıldığını', 'bildirdi'], ['gösterilerde', 'fitnecilere', 'ölüm', 'münafıklara', 'ölüm', 'abd', 'ye', 'ölüm', 'ingiltere', 'ye', 'ölüm', 'sloganları', 'atıldı'], ['dini', 'lider', 'ali', 'hamaney', 've', 'cumhurbaşkanı', 'mahmud', 'ahmedinejad', 'ı', 'destekleyen', 'iranlılar', 'son', 'olaylarda', 'yeğeni', 'öldürülen', 'mir', 'hüseyin', 'musevi', 'başta', 'olmak', 'üzere', 'muhalefet', 'liderlerini', 'kınadılar'], ['musevi', 'ye', 'ölüm', 've', 'idam', 'idam', 'sloganları', 'duyuldu'], ['muhalefet', 'liderleri', 'kaçtı', 'mı', 'aşure', 'günü', 'yaşanan', 'çatışmalarda', 'devlet', 'kaynaklarına', 'göre', 'u', 'terörist', 'olmak', 'üzere', 'kişi', 'ölmüştü']]\n",
      "CPU times: total: 28.1 s\n",
      "Wall time: 1min 26s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "corpus=[word_tokenize(i) for i in df.news]\n",
    "print(corpus[:5])\n",
    "\n",
    "# word2vec algorithm requires the whole corpus to be 2-dimensional. For this reason, we use a for loop here to extract all the documents/lines one by one and separate them into word tokens. \n",
    "# The word_tokenize function by default splits the text into word tokens and puts them in a list.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.utils import effective_n_jobs\n",
    "\n",
    "effective_n_jobs(-1)\n",
    "\n",
    "# indicates the maximum number of cores on your computer that you can use for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 10min 33s\n",
      "Wall time: 1min 40s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = Word2Vec(sentences=corpus, \n",
    "                 alpha=0.025, # learning rate\n",
    "                 vector_size=100, \n",
    "                 window=5, # how many words before and after to be considered\n",
    "                 min_count=5, # ignore tokens 5 and less in number\n",
    "                 sg=1, # skipgram, default = 0\n",
    "                 workers=12)\n",
    "\n",
    "# vector_size, this is where we specify how many dimensional word embeddings we want.\n",
    "# We specify in the window parameter how many tokens before and after this token should be taken into account when establishing semantic relationships between a token and other tokens. \n",
    "# The recommended numbers are between 5 and 15.\n",
    "# min_count, tokens with 5 or less occurrences in the corpus are not included in the training. Usually numbers like 3,4,5 are preferred.\n",
    "# sg =1, train with skipgram algorithm.\n",
    "# sg = 0, train with the CBOW algorithm.\n",
    "# alpha = learning rate\n",
    "# workers = number of cores we will use for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.09865374,  0.32249972,  0.08113028,  0.2481778 , -0.33505496,\n",
       "        0.15236615, -0.14483552,  0.8517523 , -0.3906854 , -0.32861456,\n",
       "       -0.5102848 , -0.31961584, -0.26706418,  0.47893932,  0.03734279,\n",
       "        0.12921727,  0.3474915 , -0.0190613 ,  0.16443539, -1.1152455 ,\n",
       "        0.14464793,  0.49279496,  0.35456955, -0.8454716 ,  0.24567877,\n",
       "        0.41372973, -0.44399342,  0.1175387 , -0.38418385,  0.27414685,\n",
       "        0.12816934,  0.13105349,  0.27895677,  0.16632143,  0.23622537,\n",
       "       -0.13932958, -0.00644227, -0.16935098,  0.19433688, -0.32669592,\n",
       "        0.40069288, -0.21965155,  0.36966822, -0.36889434,  0.57474166,\n",
       "        0.6174525 , -0.28649634,  0.14898083,  0.2916678 , -0.30093798,\n",
       "        0.49045202, -0.39107722, -0.15191537,  0.0565801 , -0.15135272,\n",
       "       -0.11940034,  0.22841056, -0.5871908 , -0.56219393,  0.19957703,\n",
       "        0.04338143, -0.1397902 ,  0.15929896, -0.05982132, -0.54558593,\n",
       "       -0.20749934,  0.05791085,  0.24839656,  0.03841949, -0.35422295,\n",
       "       -0.41889134,  0.01535494,  0.84237355,  0.3159725 ,  0.15311383,\n",
       "        0.13852194,  0.45811802, -0.21094956, -0.09062813, -0.02707921,\n",
       "       -0.15330723, -0.25912943, -0.09763385,  0.38340482,  0.3754287 ,\n",
       "       -0.13282323,  0.1462015 ,  0.55287826,  0.5576081 ,  0.14100917,\n",
       "        0.4749206 , -0.00758832,  0.08133811,  0.4237023 ,  0.18444002,\n",
       "       -0.3232916 , -0.02742534,  0.16491799,  0.34376726, -0.14890778],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv['ankara'] # ww = word2vec\n",
    "\n",
    "# word_embedding with 100 elements/dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('okuldaki', 0.7556234002113342),\n",
       " ('öğrenci', 0.73649001121521),\n",
       " ('öğretmeni', 0.7340482473373413),\n",
       " ('öğrenciye', 0.7300719618797302),\n",
       " ('öğretmenin', 0.7291283011436462),\n",
       " ('başörtülü', 0.7160113453865051),\n",
       " ('erkekten', 0.7152495384216309),\n",
       " ('hizmetli', 0.7111763954162598),\n",
       " ('üniversite', 0.7080069780349731),\n",
       " ('öğrenciyle', 0.7061008810997009)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar('öğretmen')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('çizgileri', 0.6615160703659058),\n",
       " ('sarı', 0.6613019108772278),\n",
       " ('gömlekliler', 0.6467803120613098),\n",
       " ('çizgi', 0.642117977142334),\n",
       " ('turuncu', 0.636613130569458),\n",
       " ('gömlekli', 0.6291239857673645),\n",
       " ('renkli', 0.6171365976333618),\n",
       " ('halıda', 0.6147487759590149),\n",
       " ('ışıkta', 0.6074832677841187),\n",
       " ('bültenle', 0.606572151184082)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar('kırmızı')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('evine', 0.839626669883728),\n",
       " ('apartmana', 0.7691282629966736),\n",
       " ('dükkana', 0.76540607213974),\n",
       " ('mağazaya', 0.7547361850738525),\n",
       " ('karakola', 0.7370243668556213),\n",
       " ('arabaya', 0.7318121790885925),\n",
       " ('hapishaneye', 0.7251378297805786),\n",
       " ('restorana', 0.7166224718093872),\n",
       " ('odasına', 0.7107588052749634),\n",
       " ('arabasına', 0.7095082402229309)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar('eve')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('marmara', 0.8875664472579956),\n",
       " ('gemisine', 0.6941417455673218),\n",
       " ('baskınıyla', 0.6774566769599915),\n",
       " ('filo', 0.6320633888244629),\n",
       " ('filosundaki', 0.6211588382720947),\n",
       " ('saldırısındaki', 0.617448627948761),\n",
       " ('baskınının', 0.6149155497550964),\n",
       " ('baskınına', 0.6054602265357971),\n",
       " ('filodaki', 0.6010278463363647),\n",
       " ('dökme', 0.5925293564796448)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar('mavi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('kaliteli', 0.6613228917121887),\n",
       " ('dersin', 0.656999409198761),\n",
       " ('psikoloji', 0.6441785097122192),\n",
       " ('almancayı', 0.6401728391647339),\n",
       " ('dersine', 0.6384567618370056),\n",
       " ('salgılanan', 0.6371878385543823),\n",
       " ('nesilden', 0.6354333162307739),\n",
       " ('doçent', 0.6344307065010071)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive=['öğrenme', 'doktor'], negative=['tedavi'], topn=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('hollanda', 0.6296952962875366),\n",
       " ('avusturya', 0.6233313083648682),\n",
       " ('fransa', 0.6201995015144348),\n",
       " ('danimarka', 0.6020408272743225),\n",
       " ('kanada', 0.5748323798179626)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive=['ankara', 'belçika'], negative=['brüksel'], topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec.load(\"word2vec.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# We use the KeyedVectors function to convert word embeddings in a different format to word2vec format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_model = 'glove.6B.100d.txt'  # trained with 6 Billion words 100 matrix dimension\n",
    "model2 = KeyedVectors.load_word2vec_format(glove_model, no_header=True) #'glove.6B.100d.txt', if no_header = False it wants the 6B.100d information \n",
    "\n",
    "# 'Word2Vec format usually has a header on the first line of the file, which contains the word count and vector size.\n",
    "# However, since our txt file is in glove format, glove.txt files do not have a header. We need to specify that there is no header information with no_header=True, otherwise you will get an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.44374 ,  0.67311 , -0.51096 ,  0.20882 , -0.10662 ,  0.55098 ,\n",
       "       -0.035593,  0.25126 , -0.32789 ,  1.0762  , -0.49637 , -0.4298  ,\n",
       "        0.36764 ,  0.57894 , -0.25027 , -0.41021 ,  0.086998, -0.16843 ,\n",
       "       -0.85764 ,  1.0404  , -1.0314  ,  0.095147,  0.30729 ,  0.12348 ,\n",
       "        0.22745 , -0.52157 , -0.72478 , -1.0843  ,  0.035966,  0.62985 ,\n",
       "       -1.0991  ,  0.67161 ,  0.33797 ,  0.14551 , -0.90049 , -0.064415,\n",
       "       -0.75247 ,  0.21741 ,  0.51594 , -0.46291 , -0.77598 ,  0.40705 ,\n",
       "        0.1889  , -0.43402 ,  0.23202 , -0.081453, -0.3882  , -0.34444 ,\n",
       "        0.080225, -0.28274 , -0.38869 , -0.58152 , -0.25558 ,  1.0027  ,\n",
       "       -0.11114 , -1.5402  , -0.16761 , -0.26558 ,  0.9325  ,  0.069397,\n",
       "        0.96618 ,  0.15449 , -0.22905 , -0.1761  ,  0.13225 , -0.55741 ,\n",
       "        0.9234  , -0.04845 ,  0.50202 ,  1.0144  , -0.1256  ,  0.30486 ,\n",
       "        0.090808,  0.17642 , -0.23146 ,  0.68386 ,  0.37269 , -0.37316 ,\n",
       "       -0.025728, -1.0279  , -0.33142 ,  0.036028, -0.24925 , -1.4405  ,\n",
       "       -1.6267  ,  0.082284, -0.080153, -0.50802 ,  0.031885, -0.60546 ,\n",
       "        0.2908  ,  0.036842,  0.55328 ,  0.66784 , -0.42574 ,  0.53331 ,\n",
       "        0.053644, -0.66522 , -0.10012 , -0.17729 ], dtype=float32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2['teacher']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('turkey', 0.7512096762657166),\n",
       " ('istanbul', 0.6787630915641785),\n",
       " ('turkish', 0.6690374612808228),\n",
       " ('damascus', 0.6372509002685547),\n",
       " ('tbilisi', 0.6322181820869446),\n",
       " ('erdogan', 0.6258037090301514),\n",
       " ('moscow', 0.6217040419578552),\n",
       " ('brussels', 0.6181437969207764),\n",
       " ('skopje', 0.6164302229881287),\n",
       " ('cyprus', 0.606403112411499)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.most_similar('ankara')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('student', 0.8083399534225464),\n",
       " ('school', 0.75455641746521),\n",
       " ('teaching', 0.7521439790725708),\n",
       " ('taught', 0.741184651851654),\n",
       " ('teachers', 0.7291542887687683),\n",
       " ('graduate', 0.7134960293769836),\n",
       " ('instructor', 0.7077120542526245),\n",
       " ('students', 0.6828974485397339),\n",
       " ('teaches', 0.6552315354347229),\n",
       " ('education', 0.6528989672660828)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.most_similar('teacher')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('physician', 0.7673240303993225),\n",
       " ('nurse', 0.75215083360672),\n",
       " ('dr.', 0.7175194025039673),\n",
       " ('doctors', 0.7080884575843811),\n",
       " ('patient', 0.7074184417724609),\n",
       " ('medical', 0.6995992660522461),\n",
       " ('surgeon', 0.6905338764190674),\n",
       " ('hospital', 0.6900930404663086),\n",
       " ('psychiatrist', 0.658909797668457),\n",
       " ('dentist', 0.6447421312332153)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.most_similar('doctor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('daughter', 0.9090957641601562)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.most_similar(positive=['woman', 'son'], negative=['man'], topn=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('mother', 0.9024618864059448)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.most_similar(positive=['woman', 'father'], negative=['man'], topn=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('aunt', 0.8368030190467834)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.most_similar(positive=['woman', 'uncle'], negative=['man'], topn=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('turkey', 0.81471186876297)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.most_similar(positive=['ankara', 'germany'], negative=['berlin'], topn=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('teacher', 0.7610154151916504)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.most_similar(positive=['teach', 'doctor'], negative=['treat'], topn=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('queen', 0.7698541283607483)]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.most_similar(positive=['woman', 'king'], negative=['man'], topn=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('lover', 0.7032662630081177)]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.most_similar(positive=['love', 'jealous'], negative=['hate'], topn=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "END OF THE PROJECT"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
