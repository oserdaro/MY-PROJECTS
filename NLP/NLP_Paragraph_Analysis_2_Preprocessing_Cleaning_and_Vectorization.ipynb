{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Paragraph Analysis: Preprocessing (Data Cleaning and Vectorization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.warn(\"this will not show\")\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the realm of data science, extracting meaningful insights from vast datasets is an art.From predictive modeling to natural language processing (NLP), the field encompasses a spectrum of techniques.Consider a scenario where a machine learning algorithm analyzes customer behavior, tokenizing text data to discern patterns in user preferences.This process involves breaking down textual information into tokens, unlocking a treasure trove of information for data scientists to explore.The synergy of NLP and data science opens doors to innovation, driving advancements in fields like recommendation systems, sentiment analysis, and beyond.\n"
     ]
    }
   ],
   "source": [
    "text = \"In the realm of data science, extracting meaningful insights from vast datasets is an art.\"\\\n",
    "\"From predictive modeling to natural language processing (NLP), the field encompasses a spectrum of techniques.\"\\\n",
    "\"Consider a scenario where a machine learning algorithm analyzes customer behavior, tokenizing text data to discern patterns in user preferences.\"\\\n",
    "\"This process involves breaking down textual information into tokens, unlocking a treasure trove of information for data scientists to explore.\"\\\n",
    "\"The synergy of NLP and data science opens doors to innovation, driving advancements in fields like recommendation systems, sentiment analysis, and beyond.\"\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "\n",
    "- `sent_tokenize`\n",
    "- `word_tokenize`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary packages\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['in the realm of data science, extracting meaningful insights from vast datasets is an art.from predictive modeling to natural language processing (nlp), the field encompasses a spectrum of techniques.consider a scenario where a machine learning algorithm analyzes customer behavior, tokenizing text data to discern patterns in user preferences.this process involves breaking down textual information into tokens, unlocking a treasure trove of information for data scientists to explore.the synergy of nlp and data science opens doors to innovation, driving advancements in fields like recommendation systems, sentiment analysis, and beyond.']\n"
     ]
    }
   ],
   "source": [
    "# Split text into sentences using NLTK\n",
    "\n",
    "sentence_token = sent_tokenize(text.lower())\n",
    "print(sentence_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-  in the realm of data science, extracting meaningful insights from vast datasets is an art.from predictive modeling to natural language processing (nlp), the field encompasses a spectrum of techniques.consider a scenario where a machine learning algorithm analyzes customer behavior, tokenizing text data to discern patterns in user preferences.this process involves breaking down textual information into tokens, unlocking a treasure trove of information for data scientists to explore.the synergy of nlp and data science opens doors to innovation, driving advancements in fields like recommendation systems, sentiment analysis, and beyond.\n"
     ]
    }
   ],
   "source": [
    "for sentence in sentence_token:\n",
    "    print('- ', sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentence_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['in', 'the', 'realm', 'of', 'data', 'science', ',', 'extracting', 'meaningful', 'insights', 'from', 'vast', 'datasets', 'is', 'an', 'art.from', 'predictive', 'modeling', 'to', 'natural', 'language', 'processing', '(', 'nlp', ')', ',', 'the', 'field', 'encompasses', 'a', 'spectrum', 'of', 'techniques.consider', 'a', 'scenario', 'where', 'a', 'machine', 'learning', 'algorithm', 'analyzes', 'customer', 'behavior', ',', 'tokenizing', 'text', 'data', 'to', 'discern', 'patterns', 'in', 'user', 'preferences.this', 'process', 'involves', 'breaking', 'down', 'textual', 'information', 'into', 'tokens', ',', 'unlocking', 'a', 'treasure', 'trove', 'of', 'information', 'for', 'data', 'scientists', 'to', 'explore.the', 'synergy', 'of', 'nlp', 'and', 'data', 'science', 'opens', 'doors', 'to', 'innovation', ',', 'driving', 'advancements', 'in', 'fields', 'like', 'recommendation', 'systems', ',', 'sentiment', 'analysis', ',', 'and', 'beyond', '.']\n"
     ]
    }
   ],
   "source": [
    "# Split text into words using NLTK\n",
    "\n",
    "word_token = word_tokenize(text.lower())\n",
    "print(word_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-  in\n",
      "-  the\n",
      "-  realm\n",
      "-  of\n",
      "-  data\n",
      "-  science\n",
      "-  ,\n",
      "-  extracting\n",
      "-  meaningful\n",
      "-  insights\n",
      "-  from\n",
      "-  vast\n",
      "-  datasets\n",
      "-  is\n",
      "-  an\n",
      "-  art.from\n",
      "-  predictive\n",
      "-  modeling\n",
      "-  to\n",
      "-  natural\n",
      "-  language\n",
      "-  processing\n",
      "-  (\n",
      "-  nlp\n",
      "-  )\n",
      "-  ,\n",
      "-  the\n",
      "-  field\n",
      "-  encompasses\n",
      "-  a\n",
      "-  spectrum\n",
      "-  of\n",
      "-  techniques.consider\n",
      "-  a\n",
      "-  scenario\n",
      "-  where\n",
      "-  a\n",
      "-  machine\n",
      "-  learning\n",
      "-  algorithm\n",
      "-  analyzes\n",
      "-  customer\n",
      "-  behavior\n",
      "-  ,\n",
      "-  tokenizing\n",
      "-  text\n",
      "-  data\n",
      "-  to\n",
      "-  discern\n",
      "-  patterns\n",
      "-  in\n",
      "-  user\n",
      "-  preferences.this\n",
      "-  process\n",
      "-  involves\n",
      "-  breaking\n",
      "-  down\n",
      "-  textual\n",
      "-  information\n",
      "-  into\n",
      "-  tokens\n",
      "-  ,\n",
      "-  unlocking\n",
      "-  a\n",
      "-  treasure\n",
      "-  trove\n",
      "-  of\n",
      "-  information\n",
      "-  for\n",
      "-  data\n",
      "-  scientists\n",
      "-  to\n",
      "-  explore.the\n",
      "-  synergy\n",
      "-  of\n",
      "-  nlp\n",
      "-  and\n",
      "-  data\n",
      "-  science\n",
      "-  opens\n",
      "-  doors\n",
      "-  to\n",
      "-  innovation\n",
      "-  ,\n",
      "-  driving\n",
      "-  advancements\n",
      "-  in\n",
      "-  fields\n",
      "-  like\n",
      "-  recommendation\n",
      "-  systems\n",
      "-  ,\n",
      "-  sentiment\n",
      "-  analysis\n",
      "-  ,\n",
      "-  and\n",
      "-  beyond\n",
      "-  .\n"
     ]
    }
   ],
   "source": [
    "for word in word_token:\n",
    "    print('- ', word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "98"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing Punctuation and Numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['in',\n",
       " 'the',\n",
       " 'realm',\n",
       " 'of',\n",
       " 'data',\n",
       " 'science',\n",
       " 'extracting',\n",
       " 'meaningful',\n",
       " 'insights',\n",
       " 'from',\n",
       " 'vast',\n",
       " 'datasets',\n",
       " 'is',\n",
       " 'an',\n",
       " 'predictive',\n",
       " 'modeling',\n",
       " 'to',\n",
       " 'natural',\n",
       " 'language',\n",
       " 'processing',\n",
       " 'nlp',\n",
       " 'the',\n",
       " 'field',\n",
       " 'encompasses',\n",
       " 'a',\n",
       " 'spectrum',\n",
       " 'of',\n",
       " 'a',\n",
       " 'scenario',\n",
       " 'where',\n",
       " 'a',\n",
       " 'machine',\n",
       " 'learning',\n",
       " 'algorithm',\n",
       " 'analyzes',\n",
       " 'customer',\n",
       " 'behavior',\n",
       " 'tokenizing',\n",
       " 'text',\n",
       " 'data',\n",
       " 'to',\n",
       " 'discern',\n",
       " 'patterns',\n",
       " 'in',\n",
       " 'user',\n",
       " 'process',\n",
       " 'involves',\n",
       " 'breaking',\n",
       " 'down',\n",
       " 'textual',\n",
       " 'information',\n",
       " 'into',\n",
       " 'tokens',\n",
       " 'unlocking',\n",
       " 'a',\n",
       " 'treasure',\n",
       " 'trove',\n",
       " 'of',\n",
       " 'information',\n",
       " 'for',\n",
       " 'data',\n",
       " 'scientists',\n",
       " 'to',\n",
       " 'synergy',\n",
       " 'of',\n",
       " 'nlp',\n",
       " 'and',\n",
       " 'data',\n",
       " 'science',\n",
       " 'opens',\n",
       " 'doors',\n",
       " 'to',\n",
       " 'innovation',\n",
       " 'driving',\n",
       " 'advancements',\n",
       " 'in',\n",
       " 'fields',\n",
       " 'like',\n",
       " 'recommendation',\n",
       " 'systems',\n",
       " 'sentiment',\n",
       " 'analysis',\n",
       " 'and',\n",
       " 'beyond']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the word tokens without punctuation and numbers\n",
    "# First way\n",
    "\n",
    "tokens_without_punc_way1 = [w for w in word_token if w.isalpha()] # .isalnum() for number and object\n",
    "tokens_without_punc_way1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "84"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokens_without_punc_way1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in the realm of data science  extracting meaningful insights from vast datasets is an art from predictive modeling to natural language processing  nlp   the field encompasses a spectrum of techniques consider a scenario where a machine learning algorithm analyzes customer behavior  tokenizing text data to discern patterns in user preferences this process involves breaking down textual information into tokens  unlocking a treasure trove of information for data scientists to explore the synergy of nlp and data science opens doors to innovation  driving advancements in fields like recommendation systems  sentiment analysis  and beyond \n",
      "------------------------------\n",
      "['in', 'the', 'realm', 'of', 'data', 'science', 'extracting', 'meaningful', 'insights', 'from', 'vast', 'datasets', 'is', 'an', 'art', 'from', 'predictive', 'modeling', 'to', 'natural', 'language', 'processing', 'nlp', 'the', 'field', 'encompasses', 'a', 'spectrum', 'of', 'techniques', 'consider', 'a', 'scenario', 'where', 'a', 'machine', 'learning', 'algorithm', 'analyzes', 'customer', 'behavior', 'tokenizing', 'text', 'data', 'to', 'discern', 'patterns', 'in', 'user', 'preferences', 'this', 'process', 'involves', 'breaking', 'down', 'textual', 'information', 'into', 'tokens', 'unlocking', 'a', 'treasure', 'trove', 'of', 'information', 'for', 'data', 'scientists', 'to', 'explore', 'the', 'synergy', 'of', 'nlp', 'and', 'data', 'science', 'opens', 'doors', 'to', 'innovation', 'driving', 'advancements', 'in', 'fields', 'like', 'recommendation', 'systems', 'sentiment', 'analysis', 'and', 'beyond']\n"
     ]
    }
   ],
   "source": [
    "# Get the word tokens without punctuation and numbers with re module\n",
    "# Second way\n",
    "\n",
    "import re\n",
    "\n",
    "tokens_without_punc_way2 = re.sub(r\"[^a-zA-Z0-9]\", \" \", text.lower())\n",
    "print(tokens_without_punc_way2)\n",
    "print('---'*10)\n",
    "tokens_without_punc_way2 = word_tokenize(tokens_without_punc_way2.lower())\n",
    "print(tokens_without_punc_way2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "92"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokens_without_punc_way2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary packages\n",
    "\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take a look at the stop words included in nltk's corpus!\n",
    "\n",
    "stop_words = stopwords.words(\"english\")\n",
    "stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['realm', 'data', 'science', 'extracting', 'meaningful', 'insights', 'vast', 'datasets', 'predictive', 'modeling', 'natural', 'language', 'processing', 'nlp', 'field', 'encompasses', 'spectrum', 'scenario', 'machine', 'learning', 'algorithm', 'analyzes', 'customer', 'behavior', 'tokenizing', 'text', 'data', 'discern', 'patterns', 'user', 'process', 'involves', 'breaking', 'textual', 'information', 'tokens', 'unlocking', 'treasure', 'trove', 'information', 'data', 'scientists', 'synergy', 'nlp', 'data', 'science', 'opens', 'doors', 'innovation', 'driving', 'advancements', 'fields', 'like', 'recommendation', 'systems', 'sentiment', 'analysis', 'beyond']\n"
     ]
    }
   ],
   "source": [
    "# Remove stop words\n",
    "\n",
    "tokens_without_stopwords = [w for w in tokens_without_punc_way1 if w not in stopwords.words(\"english\")]\n",
    "print(tokens_without_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokens_without_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['in', 'the', 'of', 'from', 'is', 'an', 'to', 'the', 'a', 'of', 'a', 'where', 'a', 'to', 'in', 'down', 'into', 'a', 'of', 'for', 'to', 'of', 'and', 'to', 'in', 'and']\n"
     ]
    }
   ],
   "source": [
    "# TODO\n",
    "# Print the stop words included in the sample text\n",
    "\n",
    "stop_words_in = [w for w in tokens_without_punc_way1 if w in stopwords.words(\"english\")]\n",
    "print(stop_words_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stop_words_in)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization\n",
    "\n",
    "- `WordNetLemmatizer()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary packages\n",
    "\n",
    "from nltk.stem.wordnet import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['realm', 'data', 'science', 'extracting', 'meaningful', 'insight', 'vast', 'datasets', 'predictive', 'modeling', 'natural', 'language', 'processing', 'nlp', 'field', 'encompasses', 'spectrum', 'scenario', 'machine', 'learning', 'algorithm', 'analyzes', 'customer', 'behavior', 'tokenizing', 'text', 'data', 'discern', 'pattern', 'user', 'process', 'involves', 'breaking', 'textual', 'information', 'token', 'unlocking', 'treasure', 'trove', 'information', 'data', 'scientist', 'synergy', 'nlp', 'data', 'science', 'open', 'door', 'innovation', 'driving', 'advancement', 'field', 'like', 'recommendation', 'system', 'sentiment', 'analysis', 'beyond']\n"
     ]
    }
   ],
   "source": [
    "# Reduce words to their root form with WordNetLemmatizer\n",
    "\n",
    "lemmed = [WordNetLemmatizer().lemmatize(w) for w in tokens_without_stopwords]\n",
    "print(lemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'realm data science extracting meaningful insight vast datasets predictive modeling natural language processing nlp field encompasses spectrum scenario machine learning algorithm analyzes customer behavior tokenizing text data discern pattern user process involves breaking textual information token unlocking treasure trove information data scientist synergy nlp data science open door innovation driving advancement field like recommendation system sentiment analysis beyond'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join(lemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "realm --> realm\n",
      "data --> data\n",
      "science --> science\n",
      "extracting --> extracting\n",
      "meaningful --> meaningful\n",
      "insights --> insight\n",
      "vast --> vast\n",
      "datasets --> datasets\n",
      "predictive --> predictive\n",
      "modeling --> modeling\n",
      "natural --> natural\n",
      "language --> language\n",
      "processing --> processing\n",
      "nlp --> nlp\n",
      "field --> field\n",
      "encompasses --> encompasses\n",
      "spectrum --> spectrum\n",
      "scenario --> scenario\n",
      "machine --> machine\n",
      "learning --> learning\n",
      "algorithm --> algorithm\n",
      "analyzes --> analyzes\n",
      "customer --> customer\n",
      "behavior --> behavior\n",
      "tokenizing --> tokenizing\n",
      "text --> text\n",
      "data --> data\n",
      "discern --> discern\n",
      "patterns --> pattern\n",
      "user --> user\n",
      "process --> process\n",
      "involves --> involves\n",
      "breaking --> breaking\n",
      "textual --> textual\n",
      "information --> information\n",
      "tokens --> token\n",
      "unlocking --> unlocking\n",
      "treasure --> treasure\n",
      "trove --> trove\n",
      "information --> information\n",
      "data --> data\n",
      "scientists --> scientist\n",
      "synergy --> synergy\n",
      "nlp --> nlp\n",
      "data --> data\n",
      "science --> science\n",
      "opens --> open\n",
      "doors --> door\n",
      "innovation --> innovation\n",
      "driving --> driving\n",
      "advancements --> advancement\n",
      "fields --> field\n",
      "like --> like\n",
      "recommendation --> recommendation\n",
      "systems --> system\n",
      "sentiment --> sentiment\n",
      "analysis --> analysis\n",
      "beyond --> beyond\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[print(x, '-->', y) for x,y in zip(tokens_without_stopwords, lemmed)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming\n",
    "\n",
    "- `PorterStemmer()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary packages\n",
    "\n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['realm', 'data', 'scienc', 'extract', 'meaning', 'insight', 'vast', 'dataset', 'predict', 'model', 'natur', 'languag', 'process', 'nlp', 'field', 'encompass', 'spectrum', 'scenario', 'machin', 'learn', 'algorithm', 'analyz', 'custom', 'behavior', 'token', 'text', 'data', 'discern', 'pattern', 'user', 'process', 'involv', 'break', 'textual', 'inform', 'token', 'unlock', 'treasur', 'trove', 'inform', 'data', 'scientist', 'synergi', 'nlp', 'data', 'scienc', 'open', 'door', 'innov', 'drive', 'advanc', 'field', 'like', 'recommend', 'system', 'sentiment', 'analysi', 'beyond']\n"
     ]
    }
   ],
   "source": [
    "# Reduce words to their stems with PorterStemmer\n",
    "\n",
    "stemmed = [PorterStemmer().stem(w) for w in tokens_without_stopwords]\n",
    "print(stemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'realm data scienc extract meaning insight vast dataset predict model natur languag process nlp field encompass spectrum scenario machin learn algorithm analyz custom behavior token text data discern pattern user process involv break textual inform token unlock treasur trove inform data scientist synergi nlp data scienc open door innov drive advanc field like recommend system sentiment analysi beyond'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join(stemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "realm --> realm\n",
      "data --> data\n",
      "science --> scienc\n",
      "extracting --> extract\n",
      "meaningful --> meaning\n",
      "insights --> insight\n",
      "vast --> vast\n",
      "datasets --> dataset\n",
      "predictive --> predict\n",
      "modeling --> model\n",
      "natural --> natur\n",
      "language --> languag\n",
      "processing --> process\n",
      "nlp --> nlp\n",
      "field --> field\n",
      "encompasses --> encompass\n",
      "spectrum --> spectrum\n",
      "scenario --> scenario\n",
      "machine --> machin\n",
      "learning --> learn\n",
      "algorithm --> algorithm\n",
      "analyzes --> analyz\n",
      "customer --> custom\n",
      "behavior --> behavior\n",
      "tokenizing --> token\n",
      "text --> text\n",
      "data --> data\n",
      "discern --> discern\n",
      "patterns --> pattern\n",
      "user --> user\n",
      "process --> process\n",
      "involves --> involv\n",
      "breaking --> break\n",
      "textual --> textual\n",
      "information --> inform\n",
      "tokens --> token\n",
      "unlocking --> unlock\n",
      "treasure --> treasur\n",
      "trove --> trove\n",
      "information --> inform\n",
      "data --> data\n",
      "scientists --> scientist\n",
      "synergy --> synergi\n",
      "nlp --> nlp\n",
      "data --> data\n",
      "science --> scienc\n",
      "opens --> open\n",
      "doors --> door\n",
      "innovation --> innov\n",
      "driving --> drive\n",
      "advancements --> advanc\n",
      "fields --> field\n",
      "like --> like\n",
      "recommendation --> recommend\n",
      "systems --> system\n",
      "sentiment --> sentiment\n",
      "analysis --> analysi\n",
      "beyond --> beyond\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[print(x, '-->', y) for x,y in zip(tokens_without_stopwords, stemmed)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|    | tokens_without_stopwords   | lemmed         | stemmed   |\n",
      "|---:|:---------------------------|:---------------|:----------|\n",
      "|  0 | realm                      | realm          | realm     |\n",
      "|  1 | data                       | data           | data      |\n",
      "|  2 | science                    | science        | scienc    |\n",
      "|  3 | extracting                 | extracting     | extract   |\n",
      "|  4 | meaningful                 | meaningful     | meaning   |\n",
      "|  5 | insights                   | insight        | insight   |\n",
      "|  6 | vast                       | vast           | vast      |\n",
      "|  7 | datasets                   | datasets       | dataset   |\n",
      "|  8 | predictive                 | predictive     | predict   |\n",
      "|  9 | modeling                   | modeling       | model     |\n",
      "| 10 | natural                    | natural        | natur     |\n",
      "| 11 | language                   | language       | languag   |\n",
      "| 12 | processing                 | processing     | process   |\n",
      "| 13 | nlp                        | nlp            | nlp       |\n",
      "| 14 | field                      | field          | field     |\n",
      "| 15 | encompasses                | encompasses    | encompass |\n",
      "| 16 | spectrum                   | spectrum       | spectrum  |\n",
      "| 17 | scenario                   | scenario       | scenario  |\n",
      "| 18 | machine                    | machine        | machin    |\n",
      "| 19 | learning                   | learning       | learn     |\n",
      "| 20 | algorithm                  | algorithm      | algorithm |\n",
      "| 21 | analyzes                   | analyzes       | analyz    |\n",
      "| 22 | customer                   | customer       | custom    |\n",
      "| 23 | behavior                   | behavior       | behavior  |\n",
      "| 24 | tokenizing                 | tokenizing     | token     |\n",
      "| 25 | text                       | text           | text      |\n",
      "| 26 | data                       | data           | data      |\n",
      "| 27 | discern                    | discern        | discern   |\n",
      "| 28 | patterns                   | pattern        | pattern   |\n",
      "| 29 | user                       | user           | user      |\n",
      "| 30 | process                    | process        | process   |\n",
      "| 31 | involves                   | involves       | involv    |\n",
      "| 32 | breaking                   | breaking       | break     |\n",
      "| 33 | textual                    | textual        | textual   |\n",
      "| 34 | information                | information    | inform    |\n",
      "| 35 | tokens                     | token          | token     |\n",
      "| 36 | unlocking                  | unlocking      | unlock    |\n",
      "| 37 | treasure                   | treasure       | treasur   |\n",
      "| 38 | trove                      | trove          | trove     |\n",
      "| 39 | information                | information    | inform    |\n",
      "| 40 | data                       | data           | data      |\n",
      "| 41 | scientists                 | scientist      | scientist |\n",
      "| 42 | synergy                    | synergy        | synergi   |\n",
      "| 43 | nlp                        | nlp            | nlp       |\n",
      "| 44 | data                       | data           | data      |\n",
      "| 45 | science                    | science        | scienc    |\n",
      "| 46 | opens                      | open           | open      |\n",
      "| 47 | doors                      | door           | door      |\n",
      "| 48 | innovation                 | innovation     | innov     |\n",
      "| 49 | driving                    | driving        | drive     |\n",
      "| 50 | advancements               | advancement    | advanc    |\n",
      "| 51 | fields                     | field          | field     |\n",
      "| 52 | like                       | like           | like      |\n",
      "| 53 | recommendation             | recommendation | recommend |\n",
      "| 54 | systems                    | system         | system    |\n",
      "| 55 | sentiment                  | sentiment      | sentiment |\n",
      "| 56 | analysis                   | analysis       | analysi   |\n",
      "| 57 | beyond                     | beyond         | beyond    |\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(np.column_stack([tokens_without_stopwords, lemmed, stemmed]), \n",
    "                  columns=['tokens_without_stopwords', 'lemmed', 'stemmed'])\n",
    "\n",
    "print(df.to_markdown())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All Together (Cleaning Function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In the realm of data science, extracting meaningful insights from vast datasets is an art.From predictive modeling to natural language processing (NLP), the field encompasses a spectrum of techniques.Consider a scenario where a machine learning algorithm analyzes customer behavior, tokenizing text data to discern patterns in user preferences.This process involves breaking down textual information into tokens, unlocking a treasure trove of information for data scientists to explore.The synergy of NLP and data science opens doors to innovation, driving advancements in fields like recommendation systems, sentiment analysis, and beyond.'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning(data):\n",
    "\n",
    "    #1. Tokenize and lower\n",
    "    text_tokens = word_tokenize(data.lower())\n",
    "\n",
    "    #2. Remove Puncs and numbers\n",
    "    tokens_without_punc = [w for w in text_tokens if w.isalpha()]\n",
    "\n",
    "    #3. Removing Stopwords\n",
    "    tokens_without_sw = [t for t in tokens_without_punc if t not in stop_words]\n",
    "\n",
    "    #4. lemma\n",
    "    text_cleaned = [WordNetLemmatizer().lemmatize(t) for t in tokens_without_sw]\n",
    "\n",
    "    #5. joining\n",
    "    return \" \".join(text_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    realm data science extracting meaningful insig...\n"
     ]
    }
   ],
   "source": [
    "# Clean the text with CLEANING function\n",
    "\n",
    "cleaned_text = pd.Series(text).apply(cleaning)\n",
    "print (cleaned_text.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "realm data science extracting meaningful insight vast datasets predictive modeling natural language processing nlp field encompasses spectrum scenario machine learning algorithm analyzes customer behavior tokenizing text data discern pattern user process involves breaking textual information token unlocking treasure trove information data scientist synergy nlp data science open door innovation driving advancement field like recommendation system sentiment analysis beyond\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cleaned_text.to_csv('output.txt', index=False)\n",
    "\n",
    "with open('output.txt', 'r') as file:\n",
    "    content = file.read()\n",
    "    print(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'realm data science extracting meaningful insight vast datasets predictive modeling natural language processing nlp field encompasses spectrum scenario machine learning algorithm analyzes customer behavior tokenizing text data discern pattern user process involves breaking textual information token unlocking treasure trove information data scientist synergy nlp data science open door innovation driving advancement field like recommendation system sentiment analysis beyond'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join(lemmed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CountVectorizer (Bag of Words)\n",
    "\n",
    "- `CountVectorizer` - Bag of Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**INSTRUCTIONS**\n",
    "\n",
    "Creating a function `tokenize` like cleaning function that takes text and applies the following:\n",
    "- convert to all lowercase,\n",
    "- punctuation removal,\n",
    "- numbers removal,\n",
    "- word tokenization, \n",
    "- lemmatization, \n",
    "- and stop word removal using `nltk`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In the realm of data science, extracting meaningful insights from vast datasets is an art.From predictive modeling to natural language processing (NLP), the field encompasses a spectrum of techniques.Consider a scenario where a machine learning algorithm analyzes customer behavior, tokenizing text data to discern patterns in user preferences.This process involves breaking down textual information into tokens, unlocking a treasure trove of information for data scientists to explore.The synergy of NLP and data science opens doors to innovation, driving advancements in fields like recommendation systems, sentiment analysis, and beyond.'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['in the realm of data science, extracting meaningful insights from vast datasets is an art.from predictive modeling to natural language processing (nlp), the field encompasses a spectrum of techniques.consider a scenario where a machine learning algorithm analyzes customer behavior, tokenizing text data to discern patterns in user preferences.this process involves breaking down textual information into tokens, unlocking a treasure trove of information for data scientists to explore.the synergy of nlp and data science opens doors to innovation, driving advancements in fields like recommendation systems, sentiment analysis, and beyond.']\n"
     ]
    }
   ],
   "source": [
    "corpus = sent_tokenize(text.lower())\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words(\"english\")\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    # normalize case and remove punctuation\n",
    "    text = re.sub(r\"[^a-zA-Z0-9]\", \" \", text.lower())\n",
    "    \n",
    "    # tokenize text\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # lemmatize andremove stop words\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary packages\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize count vectorizer object (Use TOKENIZE function as tokenizer)\n",
    "\n",
    "count_vectorizer = CountVectorizer(tokenizer=tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create X object by applying fit_transform with count_vectorizer object to corpus\n",
    "\n",
    "X = count_vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x56 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 56 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['advancement', 'algorithm', 'analysis', 'analyzes', 'art',\n",
       "       'behavior', 'beyond', 'breaking', 'consider', 'customer', 'data',\n",
       "       'datasets', 'discern', 'door', 'driving', 'encompasses', 'explore',\n",
       "       'extracting', 'field', 'information', 'innovation', 'insight',\n",
       "       'involves', 'language', 'learning', 'like', 'machine',\n",
       "       'meaningful', 'modeling', 'natural', 'nlp', 'open', 'pattern',\n",
       "       'predictive', 'preference', 'process', 'processing', 'realm',\n",
       "       'recommendation', 'scenario', 'science', 'scientist', 'sentiment',\n",
       "       'spectrum', 'synergy', 'system', 'technique', 'text', 'textual',\n",
       "       'token', 'tokenizing', 'treasure', 'trove', 'unlocking', 'user',\n",
       "       'vast'], dtype=object)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 4, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], dtype=int64)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert sparse matrix (X) to numpy array to view\n",
    "\n",
    "X.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'realm': 37,\n",
       " 'data': 10,\n",
       " 'science': 40,\n",
       " 'extracting': 17,\n",
       " 'meaningful': 27,\n",
       " 'insight': 21,\n",
       " 'vast': 55,\n",
       " 'datasets': 11,\n",
       " 'art': 4,\n",
       " 'predictive': 33,\n",
       " 'modeling': 28,\n",
       " 'natural': 29,\n",
       " 'language': 23,\n",
       " 'processing': 36,\n",
       " 'nlp': 30,\n",
       " 'field': 18,\n",
       " 'encompasses': 15,\n",
       " 'spectrum': 43,\n",
       " 'technique': 46,\n",
       " 'consider': 8,\n",
       " 'scenario': 39,\n",
       " 'machine': 26,\n",
       " 'learning': 24,\n",
       " 'algorithm': 1,\n",
       " 'analyzes': 3,\n",
       " 'customer': 9,\n",
       " 'behavior': 5,\n",
       " 'tokenizing': 50,\n",
       " 'text': 47,\n",
       " 'discern': 12,\n",
       " 'pattern': 32,\n",
       " 'user': 54,\n",
       " 'preference': 34,\n",
       " 'process': 35,\n",
       " 'involves': 22,\n",
       " 'breaking': 7,\n",
       " 'textual': 48,\n",
       " 'information': 19,\n",
       " 'token': 49,\n",
       " 'unlocking': 53,\n",
       " 'treasure': 51,\n",
       " 'trove': 52,\n",
       " 'scientist': 41,\n",
       " 'explore': 16,\n",
       " 'synergy': 44,\n",
       " 'open': 31,\n",
       " 'door': 13,\n",
       " 'innovation': 20,\n",
       " 'driving': 14,\n",
       " 'advancement': 0,\n",
       " 'like': 25,\n",
       " 'recommendation': 38,\n",
       " 'system': 45,\n",
       " 'sentiment': 42,\n",
       " 'analysis': 2,\n",
       " 'beyond': 6}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View token vocabulary and counts\n",
    "\n",
    "count_vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TfidfTransformer\n",
    "\n",
    "- `TfidfTransformer` - TF-IDF values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary packages\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize tfidf_transformer object\n",
    "\n",
    "tfidf_transformer = TfidfTransformer(smooth_idf=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply fit_transform with tfidf_transformer object to X\n",
    "\n",
    "X = tfidf_transformer.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.10976426, 0.10976426, 0.10976426, 0.10976426, 0.10976426,\n",
       "        0.10976426, 0.10976426, 0.10976426, 0.10976426, 0.10976426,\n",
       "        0.43905704, 0.10976426, 0.10976426, 0.10976426, 0.10976426,\n",
       "        0.10976426, 0.10976426, 0.10976426, 0.21952852, 0.21952852,\n",
       "        0.10976426, 0.10976426, 0.10976426, 0.10976426, 0.10976426,\n",
       "        0.10976426, 0.10976426, 0.10976426, 0.10976426, 0.10976426,\n",
       "        0.21952852, 0.10976426, 0.10976426, 0.10976426, 0.10976426,\n",
       "        0.10976426, 0.10976426, 0.10976426, 0.10976426, 0.10976426,\n",
       "        0.21952852, 0.10976426, 0.10976426, 0.10976426, 0.10976426,\n",
       "        0.10976426, 0.10976426, 0.10976426, 0.10976426, 0.10976426,\n",
       "        0.10976426, 0.10976426, 0.10976426, 0.10976426, 0.10976426,\n",
       "        0.10976426]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert sparse matrix to numpy array to view\n",
    "\n",
    "X.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TfidfVectorizer\n",
    "\n",
    "- `TfidfVectorizer` - Bag of Words AND TF-IDF values\n",
    "- `TfidfVectorizer` = `CountVectorizer` + `TfidfTransformer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necesary packages\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize `tfidf_vectorizer` object\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create `X` object by applying `fit_transform` with `tfidf_vectorizer` object to `corpus`,\n",
    "\n",
    "X = tfidf_vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['advancements', 'algorithm', 'an', 'analysis', 'analyzes', 'and',\n",
       "       'art', 'behavior', 'beyond', 'breaking', 'consider', 'customer',\n",
       "       'data', 'datasets', 'discern', 'doors', 'down', 'driving',\n",
       "       'encompasses', 'explore', 'extracting', 'field', 'fields', 'for',\n",
       "       'from', 'in', 'information', 'innovation', 'insights', 'into',\n",
       "       'involves', 'is', 'language', 'learning', 'like', 'machine',\n",
       "       'meaningful', 'modeling', 'natural', 'nlp', 'of', 'opens',\n",
       "       'patterns', 'predictive', 'preferences', 'process', 'processing',\n",
       "       'realm', 'recommendation', 'scenario', 'science', 'scientists',\n",
       "       'sentiment', 'spectrum', 'synergy', 'systems', 'techniques',\n",
       "       'text', 'textual', 'the', 'this', 'to', 'tokenizing', 'tokens',\n",
       "       'treasure', 'trove', 'unlocking', 'user', 'vast', 'where'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.08276059, 0.08276059, 0.08276059, 0.08276059, 0.08276059,\n",
       "        0.16552118, 0.08276059, 0.08276059, 0.08276059, 0.08276059,\n",
       "        0.08276059, 0.08276059, 0.33104236, 0.08276059, 0.08276059,\n",
       "        0.08276059, 0.08276059, 0.08276059, 0.08276059, 0.08276059,\n",
       "        0.08276059, 0.08276059, 0.08276059, 0.08276059, 0.16552118,\n",
       "        0.24828177, 0.16552118, 0.08276059, 0.08276059, 0.08276059,\n",
       "        0.08276059, 0.08276059, 0.08276059, 0.08276059, 0.08276059,\n",
       "        0.08276059, 0.08276059, 0.08276059, 0.08276059, 0.16552118,\n",
       "        0.33104236, 0.08276059, 0.08276059, 0.08276059, 0.08276059,\n",
       "        0.08276059, 0.08276059, 0.08276059, 0.08276059, 0.08276059,\n",
       "        0.16552118, 0.08276059, 0.08276059, 0.08276059, 0.08276059,\n",
       "        0.08276059, 0.08276059, 0.08276059, 0.08276059, 0.24828177,\n",
       "        0.08276059, 0.33104236, 0.08276059, 0.08276059, 0.08276059,\n",
       "        0.08276059, 0.08276059, 0.08276059, 0.08276059, 0.08276059]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert sparse matrix (X) to numpy array to view\n",
    "\n",
    "X.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### END OF THE PROJECT"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
